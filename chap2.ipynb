{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,1]\n",
    "\n",
    "b = (a == 1)\n",
    "**3ì¥ â€“ ë¶„ë¥˜**\n",
    "\n",
    "_ì´ ë…¸íŠ¸ë¶ì€ 3ì¥ì˜ ëª¨ë“  ìƒ˜í”Œ ì½”ë“œì™€ ì—°ìŠµ ë¬¸ì œ ì •ë‹µì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤._\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/handson-ml2/blob/master/03_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />êµ¬ê¸€ ì½”ë©ì—ì„œ ì‹¤í–‰í•˜ê¸°</a>\n",
    "  </td>\n",
    "</table>\n",
    "# ì„¤ì •\n",
    "ë¨¼ì € ëª‡ ê°œì˜ ëª¨ë“ˆì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤. ë§·í”Œë¡¯ë¦½ ê·¸ë˜í”„ë¥¼ ì¸ë¼ì¸ìœ¼ë¡œ ì¶œë ¥í•˜ë„ë¡ ë§Œë“¤ê³  ê·¸ë¦¼ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. ë˜í•œ íŒŒì´ì¬ ë²„ì „ì´ 3.5 ì´ìƒì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤(íŒŒì´ì¬ 2.xì—ì„œë„ ë™ì‘í•˜ì§€ë§Œ ê³§ ì§€ì›ì´ ì¤‘ë‹¨ë˜ë¯€ë¡œ íŒŒì´ì¬ 3ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤). ì‚¬ì´í‚·ëŸ° ë²„ì „ì´ 0.20 ì´ìƒì¸ì§€ë„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "# íŒŒì´ì¬ â‰¥3.5 í•„ìˆ˜\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# ì‚¬ì´í‚·ëŸ° â‰¥0.20 í•„ìˆ˜\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# ê³µí†µ ëª¨ë“ˆ ì„í¬íŠ¸\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ë…¸íŠ¸ë¶ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ê¸° ìœ„í•´\n",
    "np.random.seed(42)\n",
    "\n",
    "# ê¹”ë”í•œ ê·¸ë˜í”„ ì¶œë ¥ì„ ìœ„í•´\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# ê·¸ë¦¼ì„ ì €ì¥í•  ìœ„ì¹˜\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"ê·¸ë¦¼ ì €ì¥:\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "# MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.keys()\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X.shape\n",
    "y.shape\n",
    "28 * 28\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = X[0]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap=mpl.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "save_fig(\"some_digit_plot\")\n",
    "plt.show()\n",
    "y[0]\n",
    "y = y.astype(np.uint8)\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "# ìˆ«ì ê·¸ë¦¼ì„ ìœ„í•œ ì¶”ê°€ í•¨ìˆ˜\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    # n_rows = ceil(len(instances) / images_per_row) ì™€ ë™ì¼í•©ë‹ˆë‹¤:\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "\n",
    "    # í•„ìš”í•˜ë©´ ê·¸ë¦¬ë“œ ëì„ ì±„ìš°ê¸° ìœ„í•´ ë¹ˆ ì´ë¯¸ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤:\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    padded_instances = np.concatenate([instances, np.zeros((n_empty, size * size))], axis=0)\n",
    "\n",
    "    # ë°°ì—´ì˜ í¬ê¸°ë¥¼ ë°”ê¾¸ì–´ 28Ã—28 ì´ë¯¸ì§€ë¥¼ ë‹´ì€ ê·¸ë¦¬ë“œë¡œ êµ¬ì„±í•©ë‹ˆë‹¤:\n",
    "    image_grid = padded_instances.reshape((n_rows, images_per_row, size, size))\n",
    "\n",
    "    # ì¶• 0(ì´ë¯¸ì§€ ê·¸ë¦¬ë“œì˜ ìˆ˜ì§ì¶•)ê³¼ 2(ì´ë¯¸ì§€ì˜ ìˆ˜ì§ì¶•)ë¥¼ í•©ì¹˜ê³  ì¶• 1ê³¼ 3(ë‘ ìˆ˜í‰ì¶•)ì„ í•©ì¹©ë‹ˆë‹¤. \n",
    "    # ë¨¼ì € transpose()ë¥¼ ì‚¬ìš©í•´ ê²°í•©í•˜ë ¤ëŠ” ì¶•ì„ ì˜†ìœ¼ë¡œ ì´ë™í•œ ë‹¤ìŒ í•©ì¹©ë‹ˆë‹¤:\n",
    "    big_image = image_grid.transpose(0, 2, 1, 3).reshape(n_rows * size,\n",
    "                                                         images_per_row * size)\n",
    "    # í•˜ë‚˜ì˜ í° ì´ë¯¸ì§€ë¥¼ ì–»ì—ˆìœ¼ë¯€ë¡œ ì¶œë ¥í•˜ë©´ ë©ë‹ˆë‹¤:\n",
    "    plt.imshow(big_image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = X[:100]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "save_fig(\"more_digits_plot\")\n",
    "plt.show()\n",
    "y[0]\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "# ì´ì§„ ë¶„ë¥˜ê¸° í›ˆë ¨\n",
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)\n",
    "print(type(y_train))\n",
    "**ë…¸íŠ¸**: `max_iter`ì™€ `tol` ê°™ì€ ì¼ë¶€ ë§¤ê°œë³€ìˆ˜ëŠ” ì‚¬ì´í‚·ëŸ° ë‹¤ìŒ ë²„ì „ì—ì„œ ê¸°ë³¸ê°’ì´ ë°”ë€ë‹ˆë‹¤. ë²„ì „ì´ ì—…ë°ì´íŠ¸ë˜ë”ë¼ë„ ê²°ê³¼ê°€ ë°”ë€Œì§€ ì•Šë„ë¡ ì•„ì˜ˆ ë‚˜ì¤‘ì— ë°”ë€” ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•´ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ê² ìŠµë‹ˆë‹¤. ë²ˆê±°ë¡œì›€ì„ í”¼í•˜ê¸° ìœ„í•´ ì±…ì—ëŠ” ë”°ë¡œ í‘œì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "sgd_clf.predict([some_digit])\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "# ì„±ëŠ¥ ì¸¡ì •\n",
    "## êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•œ ì •í™•ë„ ì¸¡ì •\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# shuffle=Falseê°€ ê¸°ë³¸ê°’ì´ê¸° ë•Œë¬¸ì— random_stateë¥¼ ì‚­ì œí•˜ë˜ì§€ shuffle=Trueë¡œ ì§€ì •í•˜ë¼ëŠ” ê²½ê³ ê°€ ë°œìƒí•©ë‹ˆë‹¤.\n",
    "# 0.24ë²„ì „ë¶€í„°ëŠ” ì—ëŸ¬ê°€ ë°œìƒí•  ì˜ˆì •ì´ë¯€ë¡œ í–¥í›„ ë²„ì „ì„ ìœ„í•´ shuffle=Trueì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train_5[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_5[test_index]\n",
    "\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))\n",
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "ë…¸íŠ¸: ì´ ì¶œë ¥(ê·¸ë¦¬ê³  ì´ ë…¸íŠ¸ë¶ê³¼ ë‹¤ë¥¸ ë…¸íŠ¸ë¶ì˜ ì¶œë ¥)ì´ ì±…ì˜ ë‚´ìš©ê³¼ ì¡°ê¸ˆ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê±±ì •í•  í•„ìš” ì—†ìŠµë‹ˆë‹¤. ê´œì°®ìŠµë‹ˆë‹¤! ë‹¬ë¼ì§€ëŠ” ì´ìœ ê°€ ëª‡ê°€ì§€ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "* ì²«ì§¸, ì‚¬ì´í‚·ëŸ°ê³¼ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì´ ë°œì „í•˜ë©´ì„œ ì•Œê³ ë¦¬ì¦˜ì´ ì¡°ê¸ˆì”© ë³€ê²½ë˜ê¸° ë•Œë¬¸ì— ì–»ì–´ì§€ëŠ” ê²°ê´ê°’ì´ ë°”ë€” ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœì‹  ì‚¬ì´í‚·ëŸ° ë²„ì „ì„ ì‚¬ìš©í•œë‹¤ë©´(ì¼ë°˜ì ìœ¼ë¡œ ê¶Œì¥ë©ë‹ˆë‹¤) ì±…ì´ë‚˜ ì´ ë…¸íŠ¸ë¶ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©í•œ ë²„ì „ê³¼ ë‹¤ë¥¼ ê²ƒì´ë¯€ë¡œ ì°¨ì´ê°€ ë‚©ë‹ˆë‹¤. ë…¸íŠ¸ë¶ì€ ìµœì‹ ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë ¤ê³  ë…¸ë ¥í•˜ì§€ë§Œ ì±…ì˜ ë‚´ìš©ì€ ê·¸ë ‡ê²Œ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
    "* ë‘˜ì§¸, ë§ì€ í›ˆë ¨ ì•Œê³ ë¦¬ì¦˜ì€ í™•ë¥ ì ì…ë‹ˆë‹¤. ì¦‰ ë¬´ì‘ìœ„ì„±ì— ì˜ì¡´í•©ë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œ ì˜ì‚¬ ë‚œìˆ˜ë¥¼ ìƒì„±í•˜ë„ë¡ ë‚œìˆœ ìƒì„±ê¸°ì— ì‹œë“œ ê°’ì„ ì§€ì •í•˜ì—¬ ì¼ê´€ëœ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤(random_state=42ë‚˜ np.random.seed(42)ë¥¼ ì¢…ì¢… ë³´ê²Œ ë˜ëŠ” ì´ìœ ì…ë‹ˆë‹¤). í•˜ì§€ë§Œ ì—¬ê¸°ì—ì„œ ì–¸ê¸‰í•œ ë‹¤ë¥¸ ìš”ì¸ìœ¼ë¡œ ì¸í•´ ì¶©ë¶„í•˜ì§€ ì•Šì„ ë•Œê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "* ì„¸ì§¸, í›ˆë ¨ ì•Œê³ ë¦¬ì¦˜ì´ ì—¬ëŸ¬ ìŠ¤ë ˆë“œ(Cë¡œ êµ¬í˜„ëœ ì•Œê³ ë¦¬ì¦˜)ë‚˜ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤(ì˜ˆë¥¼ ë“¤ì–´ n_jobs ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ)ë¡œ ì‹¤í–‰ë˜ë©´ ì—°ì‚°ì´ ì‹¤í–‰ë˜ëŠ” ì •í™•í•œ ìˆœì„œê°€ í•­ìƒ ë³´ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê²°ê´ê°’ì´ ì¡°ê¸ˆ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "* ë§ˆì§€ë§‰ìœ¼ë¡œ, ì—¬ëŸ¬ ì„¸ì…˜ì— ê²°ì³ ìˆœì„œê°€ ë³´ì¥ë˜ì§€ ì•ŠëŠ” íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬(dict)ì´ë‚˜ ì…‹(set) ê°™ì€ ê²ƒì€ ì™„ë²½í•œ ì¬í˜„ì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë˜í•œ ë””ë ‰í† ë¦¬ ì•ˆì— ìˆëŠ” íŒŒì¼ì˜ ìˆœì„œë„ ë³´ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "## ì˜¤ì°¨ í–‰ë ¬\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred)\n",
    "y_train_perfect_predictions = y_train_5  # ì™„ë³€í•œì²™ í•˜ì\n",
    "confusion_matrix(y_train_5, y_train_perfect_predictions)\n",
    "## ì •ë°€ë„ì™€ ì¬í˜„ìœ¨\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_train_5, y_train_pred)\n",
    "cm = confusion_matrix(y_train_5, y_train_pred)\n",
    "cm[1, 1] / (cm[0, 1] + cm[1, 1])\n",
    "recall_score(y_train_5, y_train_pred)\n",
    "cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_train_5, y_train_pred)\n",
    "cm[1, 1] / (cm[1, 1] + (cm[1, 0] + cm[0, 1]) / 2)\n",
    "## ì •ë°€ë„/ì¬í˜„ìœ¨ íŠ¸ë ˆì´ë“œì˜¤í”„\n",
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "y_scores\n",
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "y_some_digit_pred\n",
    "threshold = 8000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "y_some_digit_pred\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n",
    "    plt.grid(True)                              # Not shown\n",
    "    plt.axis([-50000, 50000, 0, 1])             # Not shown\n",
    "\n",
    "\n",
    "\n",
    "recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))                                                                  # Not shown\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 # Not shown\n",
    "plt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")                                # Not shown\n",
    "plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")# Not shown\n",
    "plt.plot([threshold_90_precision], [0.9], \"ro\")                                             # Not shown\n",
    "plt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                             # Not shown\n",
    "save_fig(\"precision_recall_vs_threshold_plot\")                                              # Not shown\n",
    "plt.show()\n",
    "(y_train_pred == (y_scores > 0)).all()\n",
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.plot([recall_90_precision, recall_90_precision], [0., 0.9], \"r:\")\n",
    "plt.plot([0.0, recall_90_precision], [0.9, 0.9], \"r:\")\n",
    "plt.plot([recall_90_precision], [0.9], \"ro\")\n",
    "save_fig(\"precision_vs_recall_plot\")\n",
    "plt.show()\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "threshold_90_precision\n",
    "y_train_pred_90 = (y_scores >= threshold_90_precision)\n",
    "precision_score(y_train_5, y_train_pred_90)\n",
    "recall_score(y_train_5, y_train_pred_90)\n",
    "## ROC ê³¡ì„ \n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # ëŒ€ê° ì ì„ \n",
    "    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n",
    "    plt.grid(True)                                            # Not shown\n",
    "\n",
    "plt.figure(figsize=(8, 6))                                    # Not shown\n",
    "plot_roc_curve(fpr, tpr)\n",
    "fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]           # Not shown\n",
    "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")   # Not shown\n",
    "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")  # Not shown\n",
    "plt.plot([fpr_90], [recall_90_precision], \"ro\")               # Not shown\n",
    "save_fig(\"roc_curve_plot\")                                    # Not shown\n",
    "plt.show()\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)\n",
    "**ë…¸íŠ¸**: ì‚¬ì´í‚·ëŸ° 0.22 ë²„ì „ì—ì„œ ë°”ë€” ê¸°ë³¸ ê°’ì„ ì‚¬ìš©í•´ `n_estimators=100`ë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
    "                                    method=\"predict_proba\")\n",
    "y_scores_forest = y_probas_forest[:, 1] # ì ìˆ˜ = ì–‘ì„± í´ë˜ìŠ¤ì˜ í™•ë¥ \n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)\n",
    "recall_for_forest = tpr_forest[np.argmax(fpr_forest >= fpr_90)]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, \"b:\", linewidth=2, label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")\n",
    "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")\n",
    "plt.plot([fpr_90], [recall_90_precision], \"ro\")\n",
    "plt.plot([fpr_90, fpr_90], [0., recall_for_forest], \"r:\")\n",
    "plt.plot([fpr_90], [recall_for_forest], \"ro\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "save_fig(\"roc_curve_comparison_plot\")\n",
    "plt.show()\n",
    "roc_auc_score(y_train_5, y_scores_forest)\n",
    "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\n",
    "precision_score(y_train_5, y_train_pred_forest)\n",
    "recall_score(y_train_5, y_train_pred_forest)\n",
    "# ë‹¤ì¤‘ ë¶„ë¥˜\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
    "svm_clf.fit(X_train[:1000], y_train[:1000]) # y_train_5ì´ ì•„ë‹ˆë¼ y_trainì…ë‹ˆë‹¤\n",
    "svm_clf.predict([some_digit])\n",
    "some_digit_scores = svm_clf.decision_function([some_digit])\n",
    "some_digit_scores\n",
    "np.argmax(some_digit_scores)\n",
    "svm_clf.classes_\n",
    "svm_clf.classes_[5]\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr_clf = OneVsRestClassifier(SVC(gamma=\"auto\", random_state=42))\n",
    "ovr_clf.fit(X_train[:1000], y_train[:1000])\n",
    "ovr_clf.predict([some_digit])\n",
    "len(ovr_clf.estimators_)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit])\n",
    "sgd_clf.decision_function([some_digit])\n",
    "**ê²½ê³ **: ì‚¬ìš©í•˜ëŠ” í•˜ë“œì›¨ì–´ì— ë”°ë¼ ë‹¤ìŒ ë‘ ì…€ì„ ì‹¤í–‰í•˜ëŠ”ë° 30ë¶„ ë˜ëŠ” ê·¸ ì´ìƒ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n",
    "# ì—ëŸ¬ ë¶„ì„\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx\n",
    "ì‚¬ì´í‚·ëŸ° 0.22 ë²„ì „ë¶€í„°ëŠ” `sklearn.metrics.plot_confusion_matrix()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "def plot_confusion_matrix(matrix):\n",
    "    \"\"\"If you prefer color and a colorbar\"\"\"\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "save_fig(\"confusion_matrix_plot\", tight_layout=False)\n",
    "plt.show()\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "save_fig(\"confusion_matrix_errors_plot\", tight_layout=False)\n",
    "plt.show()\n",
    "cl_a, cl_b = 3, 5\n",
    "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
    "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
    "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
    "save_fig(\"error_analysis_digits_plot\")\n",
    "plt.show()\n",
    "# ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)\n",
    "knn_clf.predict([some_digit])\n",
    "**ê²½ê³ **: ë‹¤ìŒ ì…€ì€ ì‹¤í–‰í•˜ëŠ”ë° ë§¤ìš° ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤(í•˜ë“œì›¨ì–´ì— ë”°ë¼ ëª‡ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤).\n",
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n",
    "# ë‹¤ì¤‘ ì¶œë ¥ ë¶„ë¥˜\n",
    "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
    "X_train_mod = X_train + noise\n",
    "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
    "X_test_mod = X_test + noise\n",
    "y_train_mod = X_train\n",
    "y_test_mod = X_test\n",
    "some_index = 0\n",
    "plt.subplot(121); plot_digit(X_test_mod[some_index])\n",
    "plt.subplot(122); plot_digit(y_test_mod[some_index])\n",
    "save_fig(\"noisy_digit_example_plot\")\n",
    "plt.show()\n",
    "knn_clf.fit(X_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([X_test_mod[some_index]])\n",
    "plot_digit(clean_digit)\n",
    "save_fig(\"cleaned_digit_example_plot\")\n",
    "# ì¶”ê°€ ë‚´ìš©\n",
    "## ë”ë¯¸ (ì¦‰ ëœë¤) ë¶„ë¥˜ê¸°\n",
    "from sklearn.dummy import DummyClassifier\n",
    "# 0.24ë²„ì „ë¶€í„° strategyì˜ ê¸°ë³¸ê°’ì´ 'stratified'ì—ì„œ 'prior'ë¡œ ë°”ë€Œë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "dmy_clf = DummyClassifier(strategy='prior')\n",
    "y_probas_dmy = cross_val_predict(dmy_clf, X_train, y_train_5, cv=3, method=\"predict_proba\")\n",
    "y_scores_dmy = y_probas_dmy[:, 1]\n",
    "fprr, tprr, thresholdsr = roc_curve(y_train_5, y_scores_dmy)\n",
    "plot_roc_curve(fprr, tprr)\n",
    "## KNN ë¶„ë¥˜ê¸°\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(weights='distance', n_neighbors=4)\n",
    "knn_clf.fit(X_train, y_train)\n",
    "y_knn_pred = knn_clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_knn_pred)\n",
    "from scipy.ndimage.interpolation import shift\n",
    "def shift_digit(digit_array, dx, dy, new=0):\n",
    "    return shift(digit_array.reshape(28, 28), [dy, dx], cval=new).reshape(784)\n",
    "\n",
    "plot_digit(shift_digit(some_digit, 5, 1, new=100))\n",
    "X_train_expanded = [X_train]\n",
    "y_train_expanded = [y_train]\n",
    "for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
    "    shifted_images = np.apply_along_axis(shift_digit, axis=1, arr=X_train, dx=dx, dy=dy)\n",
    "    X_train_expanded.append(shifted_images)\n",
    "    y_train_expanded.append(y_train)\n",
    "\n",
    "X_train_expanded = np.concatenate(X_train_expanded)\n",
    "y_train_expanded = np.concatenate(y_train_expanded)\n",
    "X_train_expanded.shape, y_train_expanded.shape\n",
    "knn_clf.fit(X_train_expanded, y_train_expanded)\n",
    "y_knn_expanded_pred = knn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_knn_expanded_pred)\n",
    "ambiguous_digit = X_test[2589]\n",
    "knn_clf.predict_proba([ambiguous_digit])\n",
    "plot_digit(ambiguous_digit)\n",
    "# ì—°ìŠµë¬¸ì œ í•´ë‹µ\n",
    "## 1. 97% ì •í™•ë„ì˜ MNIST ë¶„ë¥˜ê¸°\n",
    "**ê²½ê³ **: ì‚¬ìš©í•˜ëŠ” í•˜ë“œì›¨ì–´ì— ë”°ë¼ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ëŠ”ë° 16ì‹œê°„ ë˜ëŠ” ê·¸ ì´ìƒ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "## 2. ë°ì´í„° ì¦ì‹\n",
    "from scipy.ndimage.interpolation import shift\n",
    "def shift_image(image, dx, dy):\n",
    "    image = image.reshape((28, 28))\n",
    "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
    "    return shifted_image.reshape([-1])\n",
    "image = X_train[1000]\n",
    "shifted_image_down = shift_image(image, 0, 5)\n",
    "shifted_image_left = shift_image(image, -5, 0)\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(131)\n",
    "plt.title(\"Original\", fontsize=14)\n",
    "plt.imshow(image.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "plt.subplot(132)\n",
    "plt.title(\"Shifted down\", fontsize=14)\n",
    "plt.imshow(shifted_image_down.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "plt.subplot(133)\n",
    "plt.title(\"Shifted left\", fontsize=14)\n",
    "plt.imshow(shifted_image_left.reshape(28, 28), interpolation=\"nearest\", cmap=\"Greys\")\n",
    "plt.show()\n",
    "X_train_augmented = [image for image in X_train]\n",
    "y_train_augmented = [label for label in y_train]\n",
    "\n",
    "for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
    "    for image, label in zip(X_train, y_train):\n",
    "        X_train_augmented.append(shift_image(image, dx, dy))\n",
    "        y_train_augmented.append(label)\n",
    "\n",
    "X_train_augmented = np.array(X_train_augmented)\n",
    "y_train_augmented = np.array(y_train_augmented)\n",
    "shuffle_idx = np.random.permutation(len(X_train_augmented))\n",
    "X_train_augmented = X_train_augmented[shuffle_idx]\n",
    "y_train_augmented = y_train_augmented[shuffle_idx]\n",
    "knn_clf = KNeighborsClassifier(**grid_search.best_params_)\n",
    "knn_clf.fit(X_train_augmented, y_train_augmented)\n",
    "**ê²½ê³ **: ì‚¬ìš©í•˜ëŠ” í•˜ë“œì›¨ì–´ì— ë”°ë¼ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ëŠ”ë° 1ì‹œê°„ ë˜ëŠ” ê·¸ ì´ìƒ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "y_pred = knn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "ê°„ë‹¨íˆ ë°ì´í„°ë¥¼ ì¦ì‹í•´ì„œ 0.5% ì •í™•ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤. :)\n",
    "## 3. íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë„ì „\n",
    "ìŠ¹ê°ì˜ ë‚˜ì´, ì„±ë³„, ìŠ¹ê° ë“±ê¸‰, ìŠ¹ì„  ìœ„ì¹˜ ê°™ì€ ì†ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ìŠ¹ê°ì˜ ìƒì¡´ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.\n",
    "ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ ë³´ì£ :\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "TITANIC_PATH = os.path.join(\"datasets\", \"titanic\")\n",
    "DOWNLOAD_URL = \"https://raw.githubusercontent.com/rickiepark/handson-ml2/master/datasets/titanic/\"\n",
    "\n",
    "def fetch_titanic_data(url=DOWNLOAD_URL, path=TITANIC_PATH):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    for filename in (\"train.csv\", \"test.csv\"):\n",
    "        filepath = os.path.join(path, filename)\n",
    "        if not os.path.isfile(filepath):\n",
    "            print(\"Downloading\", filename)\n",
    "            urllib.request.urlretrieve(url + filename, filepath)\n",
    "\n",
    "fetch_titanic_data()   \n",
    "import pandas as pd\n",
    "\n",
    "def load_titanic_data(filename, titanic_path=TITANIC_PATH):\n",
    "    csv_path = os.path.join(titanic_path, filename)\n",
    "    return pd.read_csv(csv_path)\n",
    "train_data = load_titanic_data(\"train.csv\")\n",
    "test_data = load_titanic_data(\"test.csv\")\n",
    "ë°ì´í„°ëŠ” ì´ë¯¸ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë ˆì´ë¸”ì„ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤: í›ˆë ¨ ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ê°€ëŠ¥í•œ ìµœê³ ì˜ ëª¨ë¸ì„ ë§Œë“¤ê³  í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìºê¸€(Kaggle)ì— ì—…ë¡œë“œí•˜ì—¬ ìµœì¢… ì ìˆ˜ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.\n",
    "í›ˆë ¨ ì„¸íŠ¸ì—ì„œ ë§¨ ìœ„ ëª‡ ê°œì˜ ì—´ì„ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "train_data.head()\n",
    "ì†ì„±ì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤:\n",
    "* **PassengerId**: ê° ìŠ¹ê°ì˜ ê³ ìœ  ì‹ë³„ì.\n",
    "* **Survived**: íƒ€ê¹ƒì…ë‹ˆë‹¤. 0ì€ ìƒì¡´í•˜ì§€ ëª»í•œ ê²ƒì´ê³  1ì€ ìƒì¡´ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "* **Pclass**: ìŠ¹ê° ë“±ê¸‰. 1, 2, 3ë“±ì„.\n",
    "* **Name**, **Sex**, **Age**: ì´ë¦„ ê·¸ëŒ€ë¡œ ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "* **SibSp**: í•¨ê»˜ íƒ‘ìŠ¹í•œ í˜•ì œ, ë°°ìš°ìì˜ ìˆ˜.\n",
    "* **Parch**: í•¨ê»˜ íƒ‘ìŠ¹í•œ ìë…€, ë¶€ëª¨ì˜ ìˆ˜.\n",
    "* **Ticket**: í‹°ì¼“ ì•„ì´ë””\n",
    "* **Fare**: í‹°ì¼“ ìš”ê¸ˆ (íŒŒìš´ë“œ)\n",
    "* **Cabin**: ê°ì‹¤ ë²ˆí˜¸\n",
    "* **Embarked**: ìŠ¹ê°ì´ íƒ‘ìŠ¹í•œ ê³³. C(Cherbourg), Q(Queenstown), S(Southampton)\n",
    "`PassengerId` ì—´ì„ ì¸ë±ìŠ¤ ì—´ë¡œ ì§€ì •í•˜ê² ìŠµë‹ˆë‹¤:\n",
    "train_data = train_data.set_index(\"PassengerId\")\n",
    "test_data = test_data.set_index(\"PassengerId\")\n",
    "ëˆ„ë½ëœ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ë˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "train_data.info()\n",
    "train_data[train_data[\"Sex\"]==\"female\"][\"Age\"].median()\n",
    "ì¢‹ìŠµë‹ˆë‹¤. **Age**, **Cabin**, **Embarked** ì†ì„±ì˜ ì¼ë¶€ê°€ nullì…ë‹ˆë‹¤(891ê°œì˜ non-null ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤). íŠ¹íˆ **Cabin**ì€ 77%ê°€ nullì…ë‹ˆë‹¤. ì¼ë‹¨ **Cabin**ì€ ë¬´ì‹œí•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ í™œìš©í•˜ê² ìŠµë‹ˆë‹¤. **Age**ëŠ” 19%ê°€ nullì´ë¯€ë¡œ ì´ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤. nullì„ ì¤‘ê°„ ë‚˜ì´ë¡œ ë°”ê¾¸ëŠ” ê²ƒì´ ê´œì°®ì•„ ë³´ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì—´ì„ ì‚¬ìš©í•˜ì—¬ ë‚˜ì´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ(ì˜ˆë¥¼ ë“¤ì–´, 1ë“±ì„ì˜ ì¤‘ê°„ ë‚˜ì´ëŠ” 37, 2ë“±ì„ì€ 29, 3ë“±ì„ì€ 24ì…ë‹ˆë‹¤)ì´ ì¡°ê¸ˆ í˜„ëª…í•´ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ ë‹¨ìˆœí•¨ì„ ìœ„í•´ ì „ì²´ì˜ ì¤‘ê°„ ë‚˜ì´ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "**Name**ê³¼ **Ticket** ì†ì„±ë„ ê°’ì„ ê°€ì§€ê³  ìˆì§€ë§Œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ ì¡°ê¸ˆ ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì§€ê¸ˆì€ ì´ ë‘ ì†ì„±ì„ ë¬´ì‹œí•˜ê² ìŠµë‹ˆë‹¤.\n",
    "í†µê³„ì¹˜ë¥¼ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "train_data.describe()\n",
    "* ì´í¬, 38%ë§Œ **Survived**ì…ë‹ˆë‹¤. ğŸ˜­ ê±°ì˜ 40%ì— ê°€ê¹Œìš°ë¯€ë¡œ ì •í™•ë„ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ í‰ê°€í•´ë„ ê´œì°®ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
    "* í‰ê·  **Fare**ëŠ” 32.20 íŒŒìš´ë“œë¼ ê·¸ë ‡ê²Œ ë¹„ì‹¸ë³´ì´ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤(ì•„ë§ˆ ìš”ê¸ˆì„ ë§ì´ ë°˜í™˜í•´ ì£¼ì—ˆê¸° ë•Œë¬¸ì¼ ê²ƒì…ë‹ˆë‹¤)\n",
    "* í‰ê·  **Age**ëŠ” 30ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.\n",
    "íƒ€ê¹ƒì´ 0ê³¼ 1ë¡œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:\n",
    "train_data[\"Survived\"].value_counts()\n",
    "ë²”ì£¼í˜• íŠ¹ì„±ë“¤ì„ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "train_data[\"Pclass\"].value_counts()\n",
    "train_data[\"Sex\"].value_counts()\n",
    "train_data[\"Embarked\"].value_counts()\n",
    "**Embarked** íŠ¹ì„±ì€ ìŠ¹ê°ì´ íƒ‘ìŠ¹í•œ ê³³ì„ ì•Œë ¤ ì¤ë‹ˆë‹¤: C=Cherbourg, Q=Queenstown, S=Southampton.\n",
    "ìˆ˜ì¹˜ íŠ¹ì„±ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ë¶€í„° ì‹œì‘í•´ì„œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ì–´ ë³´ì£ :\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "ì´ì œ ë²”ì£¼í˜• íŠ¹ì„±ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“­ë‹ˆë‹¤:\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n",
    "    ])\n",
    "ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ«ìì™€ ë²”ì£¼í˜• íŒŒì´í”„ë¼ì¸ì„ ì—°ê²°í•©ë‹ˆë‹¤:\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "cat_attribs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "\n",
    "preprocess_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "ì¢‹ìŠµë‹ˆë‹¤! ì´ì œ ì›ë³¸ ë°ì´í„°ë¥¼ ë°›ì•„ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì£¼ì…í•  ìˆ«ì ì…ë ¥ íŠ¹ì„±ì„ ì¶œë ¥í•˜ëŠ” ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.\n",
    "X_train = preprocess_pipeline.fit_transform(\n",
    "    train_data[num_attribs + cat_attribs])\n",
    "X_train\n",
    "ë ˆì´ë¸”ì„ ê°€ì ¸ì˜µë‹ˆë‹¤:\n",
    "y_train = train_data[\"Survived\"]\n",
    "ì´ì œ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œí‚¬ ì°¨ë¡€ì…ë‹ˆë‹¤. ë¨¼ì € `RandomForestClassifier`ë¥¼ ì‚¬ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "ëª¨ë¸ì´ ì˜ í›ˆë ¨ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ë¥¼ ì‚¬ìš©í•´ì„œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ë§Œë“­ë‹ˆë‹¤:\n",
    "X_test = preprocess_pipeline.transform(test_data[num_attribs + cat_attribs])\n",
    "y_pred = forest_clf.predict(X_test)\n",
    "ì´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ (ìºê¸€ì—ì„œ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœì¸) CSV íŒŒì¼ë¡œ ë§Œë“¤ì–´ ì—…ë¡œë“œí•˜ê³  í‰ê°€ë¥¼ ë°›ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ëƒ¥ ì¢‹ì„ê±°ë¼ ê¸°ëŒ€í•˜ëŠ” ê²ƒë³´ë‹¤ êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ í‰ê°€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\n",
    "forest_scores.mean()\n",
    "ì•„ì£¼ ë‚˜ì˜ì§€ ì•Šë„¤ìš”! ìºê¸€ì—ì„œ íƒ€ì´íƒ€ë‹‰ ê²½ì—° ëŒ€íšŒì˜ [ë¦¬ë”ë³´ë“œ](https://www.kaggle.com/c/titanic/leaderboard)ì—ì„œ ìƒìœ„ 2% ì•ˆì— ë“  ì ìˆ˜ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™€ìš°! ì–´ë–¤ ì‚¬ëŒë“¤ì€ 100%ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ íƒ€ì´íƒ€ë‹‰ì˜ [í¬ìƒì ëª©ë¡](https://www.encyclopedia-titanica.org/titanic-victims/)ì„ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¨¸ì‹ ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ ë„ ì´ëŸ° ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ˜†\n",
    "`SVC`ë¥¼ ì ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(gamma=\"auto\")\n",
    "svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\n",
    "svm_scores.mean()\n",
    "ì¢‹ë„¤ìš”! ì´ ëª¨ë¸ì´ í›¨ì”¬ ë‚˜ì•„ë³´ì…ë‹ˆë‹¤.\n",
    "í•˜ì§€ë§Œ 10 í´ë“œ êµì°¨ ê²€ì¦ì— ëŒ€í•œ í‰ê·  ì •í™•ë„ë¥¼ ë³´ëŠ” ëŒ€ì‹  ëª¨ë¸ì—ì„œ ì–»ì€ 10ê°œì˜ ì ìˆ˜ë¥¼ 1ì‚¬ë¶„ìœ„, 3ì‚¬ë¶„ìœ„ë¥¼ ëª…ë£Œí•˜ê²Œ í‘œí˜„í•´ì£¼ëŠ” ìƒì ìˆ˜ì—¼ ê·¸ë¦¼(box-and-whisker) ê·¸ë˜í”„ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤(ì´ ë°©ì‹ì„ ì œì•ˆí•´ ì¤€ Nevin Yilmazì—ê²Œ ê°ì‚¬í•©ë‹ˆë‹¤). `boxplot()` í•¨ìˆ˜ëŠ” ì´ìƒì¹˜(í”Œë¼ì´ì–´(flier)ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤)ë¥¼ ê°ì§€í•˜ê³  ìˆ˜ì—¼ ë¶€ë¶„ì— ì´ë¥¼ í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤. 1ì‚¬ë¶„ìœ„ê°€ $Q_1$ì´ê³  3ì‚¬ë¶„ìœ„ê°€ $Q_3$ì´ë¼ë©´ ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„ëŠ” $IQR = Q_3 - Q_1$ê°€ ë©ë‹ˆë‹¤(ì´ ê°’ì´ ë°•ìŠ¤ì˜ ë†’ì´ê°€ ë©ë‹ˆë‹¤). $Q_1 - 1.5 \\times IQR$ ë³´ë‹¤ ë‚®ê±°ë‚˜ $Q3 + 1.5 \\times IQR$ ë³´ë‹¤ ë†’ì€ ì ìˆ˜ëŠ” ì´ìƒì¹˜ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot([1]*10, svm_scores, \".\")\n",
    "plt.plot([2]*10, forest_scores, \".\")\n",
    "plt.boxplot([svm_scores, forest_scores], labels=(\"SVM\",\"Random Forest\"))\n",
    "plt.ylabel(\"Accuracy\", fontsize=14)\n",
    "plt.show()\n",
    "ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸°ê°€ 10ê°œì˜ í´ë“œ ì¤‘ í•˜ë‚˜ì—ì„œ ë§¤ìš° ë†’ì€ ì ìˆ˜ë¥¼ ì–»ì—ˆì§€ë§Œ ë„“ê²Œ í¼ì ¸ ìˆê¸° ë•Œë¬¸ì— ì „ì²´ì ì¸ í‰ê·  ì ìˆ˜ëŠ” ë‚®ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ SVM ë¶„ë¥˜ê¸°ê°€ ì¼ë°˜í™”ë¥¼ ë” ì˜í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
    "ì´ ê²°ê³¼ë¥¼ ë” í–¥ìƒì‹œí‚¤ë ¤ë©´:\n",
    "* êµì°¨ ê²€ì¦ê³¼ ê·¸ë¦¬ë“œ íƒìƒ‰ì„ ì‚¬ìš©í•˜ì—¬ ë” ë§ì€ ëª¨ë¸ì„ ë¹„êµí•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ì„¸ìš”.\n",
    "* íŠ¹ì„± ê³µí•™ì„ ë” ì‹œë„í•´ ë³´ì„¸ìš”, ì˜ˆë¥¼ ë“¤ë©´:\n",
    "  * ìˆ˜ì¹˜ íŠ¹ì„±ì„ ë²”ì£¼í˜• íŠ¹ì„±ìœ¼ë¡œ ë°”ê¾¸ì–´ ë³´ì„¸ìš”: ì˜ˆë¥¼ ë“¤ì–´, ë‚˜ì´ëŒ€ê°€ ë‹¤ë¥¸ ê²½ìš° ë‹¤ë¥¸ ìƒì¡´ ë¹„ìœ¨ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì•„ë˜ ì°¸ì¡°). ê·¸ëŸ¬ë¯€ë¡œ ë‚˜ì´ êµ¬ê°„ì„ ë²”ì£¼ë¡œ ë§Œë“¤ì–´ ë‚˜ì´ ëŒ€ì‹  ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠ¤ë‹ˆë‹¤. ë¹„ìŠ·í•˜ê²Œ ìƒì¡´ìì˜ 30%ê°€ í˜¼ì ì—¬í–‰í•˜ëŠ” ì‚¬ëŒì´ê¸° ë•Œë¬¸ì— ì´ë“¤ì„ ìœ„í•œ íŠ¹ë³„í•œ ë²”ì£¼ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ì•„ë˜ ì°¸ì¡°).\n",
    "  * **SibSp**ì™€ **Parch**ì„ ì´ ë‘ íŠ¹ì„±ì˜ í•©ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "  * **Survived** íŠ¹ì„±ê³¼ ê´€ë ¨ëœ ì´ë¦„ì„ êµ¬ë³„í•´ ë³´ì„¸ìš”.\n",
    "  * **Cabin** ì—´ì„ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ ì²« ê¸€ìë¥¼ ë²”ì£¼í˜• ì†ì„±ì²˜ëŸ¼ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "train_data[\"AgeBucket\"] = train_data[\"Age\"] // 15 * 15\n",
    "train_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()\n",
    "train_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n",
    "train_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()\n",
    "## 4. ìŠ¤íŒ¸ í•„í„°\n",
    "ë¨¼ì € ë°ì´í„°ë¥¼ ë‹¤ìš´ë°›ìŠµë‹ˆë‹¤:\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=spam_path)\n",
    "        tar_bz2_file.close()\n",
    "fetch_spam_data()\n",
    "ë‹¤ìŒ, ëª¨ë“  ì´ë©”ì¼ì„ ì½ì–´ ë“¤ì…ë‹ˆë‹¤:\n",
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]\n",
    "len(ham_filenames)\n",
    "len(spam_filenames)\n",
    "íŒŒì´ì¬ì˜ `email` ëª¨ë“ˆì„ ì‚¬ìš©í•´ ì´ë©”ì¼ì„ íŒŒì‹±í•©ë‹ˆë‹¤(í—¤ë”, ì¸ì½”ë”© ë“±ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤):\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]\n",
    "ë°ì´í„°ê°€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ ê°ì„ ì¡ê¸° ìœ„í•´ í–„ ë©”ì¼ê³¼ ìŠ¤íŒ¸ ë©”ì¼ì„ í•˜ë‚˜ì”© ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "print(ham_emails[1].get_content().strip())\n",
    "print(spam_emails[6].get_content().strip())\n",
    "ì–´ë–¤ ì´ë©”ì¼ì€ ì´ë¯¸ì§€ë‚˜ ì²¨ë¶€ íŒŒì¼ì„ ê°€ì§„ ë©€í‹°íŒŒíŠ¸(multipart)ì…ë‹ˆë‹¤(ë©”ì¼ì— í¬í•¨ë˜ì–´ ìˆì„ìˆ˜ ìˆìŠµë‹ˆë‹¤). ì–´ë–¤ íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures\n",
    "structures_counter(ham_emails).most_common()\n",
    "structures_counter(spam_emails).most_common()\n",
    "í–„ ë©”ì¼ì€ í‰ë²”í•œ í…ìŠ¤íŠ¸ê°€ ë§ê³  ìŠ¤íŒ¸ì€ HTMLì¼ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì ì€ ìˆ˜ì˜ í–„ ì´ë©”ì¼ì´ PGPë¡œ ì„œëª…ë˜ì–´ ìˆì§€ë§Œ ìŠ¤íŒ¸ ë©”ì¼ì—ëŠ” ì—†ìŠµë‹ˆë‹¤. ìš”ì•½í•˜ë©´ ì´ë©”ì¼ êµ¬ì¡°ëŠ” ìœ ìš©í•œ ì •ë³´ì…ë‹ˆë‹¤.\n",
    "ì´ì œ ì´ë©”ì¼ í—¤ë”ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "for header, value in spam_emails[0].items():\n",
    "    print(header,\":\",value)\n",
    "ë³´ë‚¸ì‚¬ëŒì˜ ì´ë©”ì¼ ì£¼ì†Œì™€ ê°™ì´ í—¤ë”ì—ëŠ” ìœ ìš©í•œ ì •ë³´ê°€ ë§ì´ ìˆì§€ë§Œ ì—¬ê¸°ì„œëŠ” `Subject` í—¤ë”ë§Œ ë‹¤ë¤„ ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "spam_emails[0][\"Subject\"]\n",
    "ì¢‹ìŠµë‹ˆë‹¤. ë°ì´í„°ì—ë¥¼ ë” ì‚´í´ë³´ê¸° ì „ì— í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤:\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype=object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "ì´ì œ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤. ë¨¼ì € HTMLì„ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì—ëŠ” ë‹¹ì—°íˆ [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ì¢‹ì§€ë§Œ ì˜ì¡´ì„±ì„ ì¤„ì´ê¸° ìœ„í•´ì„œ ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê°• ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤([unÌ¨hoÍly radianÍceÍ destroÒ‰ying all enliÌÍ„Ì‚Í„ghtenment](https://stackoverflow.com/a/1732454/38626)ì˜ ìœ„í—˜ì—ë„ ë¶ˆêµ¬í•˜ê³ ). ë‹¤ìŒ í•¨ìˆ˜ëŠ” `<head>` ì„¹ì…˜ì„ ì‚­ì œí•˜ê³  ëª¨ë“  `<a>` íƒœê·¸ë¥¼ HYPERLINK ë¬¸ìë¡œ ë°”ê¿‰ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ëª¨ë“  HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¹ë‹ˆë‹¤. ë³´ê¸° í¸í•˜ê²Œ ì—¬ëŸ¬ê°œì˜ ê°œí–‰ ë¬¸ìë¥¼ í•˜ë‚˜ë¡œ ë§Œë“¤ê³  (`&gt;`ë‚˜ `&nbsp;` ê°™ì€) html ì—”í‹°í‹°ë¥¼ ë³µì›í•©ë‹ˆë‹¤:\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)\n",
    "ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒì€ HTML ìŠ¤íŒ¸ì…ë‹ˆë‹¤:\n",
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")\n",
    "ë³€í™˜ëœ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤:\n",
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")\n",
    "ì•„ì£¼ ì¢‹ìŠµë‹ˆë‹¤! ì´ì œ í¬ë§·ì— ìƒê´€ì—†ì´ ì´ë©”ì¼ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤:\n",
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)\n",
    "print(email_to_text(sample_html_spam)[:100], \"...\")\n",
    "ì–´ê°„ ì¶”ì¶œì„ í•´ë³´ì£ ! ì´ ì‘ì—…ì„ í•˜ë ¤ë©´ ìì—°ì–´ ì²˜ë¦¬ íˆ´í‚·([NLTK](http://www.nltk.org/))ì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ê°„ë‹¨íˆ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ë¨¼ì € virtualenv í™˜ê²½ì„ í™œì„±í™”ì‹œì¼œì•¼ í•©ë‹ˆë‹¤. ë³„ë„ì˜ í™˜ê²½ì´ ì—†ë‹¤ë©´ ì–´ë“œë¯¼ ê¶Œí•œì´ í•„ìš”í• ì§€ ëª¨ë¦…ë‹ˆë‹¤. ì•„ë‹ˆë©´ `--user` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”):\n",
    "\n",
    "`$ pip install nltk`\n",
    "try:\n",
    "    import nltk\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))\n",
    "except ImportError:\n",
    "    print(\"Error: stemming requires the NLTK module.\")\n",
    "    stemmer = None\n",
    "ì¸í„°ë„· ì£¼ì†ŒëŠ” \"URL\" ë¬¸ìë¡œ ë°”ê¾¸ê² ìŠµë‹ˆë‹¤. [ì •ê·œì‹](https://mathiasbynens.be/demo/url-regex)ì„ í•˜ë“œ ì½”ë”©í•  ìˆ˜ë„ ìˆì§€ë§Œ [urlextract](https://github.com/lipoja/URLExtract) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤(ë¨¼ì € virtualenv í™˜ê²½ì„ í™œì„±í™”ì‹œì¼œì•¼ í•©ë‹ˆë‹¤. ë³„ë„ì˜ í™˜ê²½ì´ ì—†ë‹¤ë©´ ì–´ë“œë¯¼ ê¶Œí•œì´ í•„ìš”í• ì§€ ëª¨ë¦…ë‹ˆë‹¤. ì•„ë‹ˆë©´ `--user` ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”):\n",
    "\n",
    "`$ pip install urlextract`\n",
    "# ì½”ë©ì—ì„œ ì´ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ë ¤ë©´ ë¨¼ì € pip install urlextractì„ ì‹¤í–‰í•©ë‹ˆë‹¤\n",
    "try:\n",
    "    import google.colab\n",
    "    %pip install -q -U urlextract\n",
    "except ImportError:\n",
    "    pass # ì½”ë©ì—ì„œëŠ” ì‹¤í–‰ë˜ì§€ ì•ŠìŒ\n",
    "**ë…¸íŠ¸:** ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œëŠ” í•­ìƒ `!pip` ëŒ€ì‹  `%pip`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. `!pip`ëŠ” ë‹¤ë¥¸ í™˜ê²½ì— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë°˜ë©´ `%pip`ëŠ” í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ í™˜ê²½ì— ì„¤ì¹˜ë©ë‹ˆë‹¤.\n",
    "try:\n",
    "    import urlextract # ë£¨íŠ¸ ë„ë©”ì¸ ì´ë¦„ì„ ë‹¤ìš´ë¡œë“œí•˜ê¸° ìœ„í•´ ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í• ì§€ ëª¨ë¦…ë‹ˆë‹¤\n",
    "    \n",
    "    url_extractor = urlextract.URLExtract()\n",
    "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
    "except ImportError:\n",
    "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
    "    url_extractor = None\n",
    "ì´ë“¤ì„ ëª¨ë‘ í•˜ë‚˜ì˜ ë³€í™˜ê¸°ë¡œ ì—°ê²°í•˜ì—¬ ì´ë©”ì¼ì„ ë‹¨ì–´ ì¹´ìš´íŠ¸ë¡œ ë°”ê¿€ ê²ƒì…ë‹ˆë‹¤. íŒŒì´ì¬ì˜ `split()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ êµ¬ë‘£ì ê³¼ ë‹¨ì–´ ê²½ê³„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ ë°”ê¿‰ë‹ˆë‹¤. ì´ ë°©ë²•ì´ ë§ì€ ì–¸ì–´ì— í†µí•˜ì§€ë§Œ ì „ë¶€ëŠ” ì•„ë‹™ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì¤‘êµ­ì–´ì™€ ì¼ë³¸ì–´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¨ì–´ ì‚¬ì´ì— ê³µë°±ì„ ë‘ì§€ ì•ŠìŠµë‹ˆë‹¤. ë² íŠ¸ë‚¨ì–´ëŠ” ìŒì ˆ ì‚¬ì´ì— ê³µë°±ì„ ë‘ê¸°ë„ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë°ì´í„°ì…‹ì´ (ê±°ì˜) ì˜ì–´ë¡œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ë¬¸ì œì—†ìŠµë‹ˆë‹¤.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)\n",
    "ì´ ë³€í™˜ê¸°ë¥¼ ëª‡ ê°œì˜ ì´ë©”ì¼ì— ì ìš©í•´ ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts\n",
    "ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ” ê²ƒ ê°™ë„¤ìš”!\n",
    "ì´ì œ ë‹¨ì–´ ì¹´ìš´íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œ ë˜ ë‹¤ë¥¸ ë³€í™˜ê¸°ë¥¼ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. ì´ ë³€í™˜ê¸°ëŠ” (ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ìˆœìœ¼ë¡œ ì •ë ¬ëœ) ì–´íœ˜ ëª©ë¡ì„ êµ¬ì¶•í•˜ëŠ” `fit()` ë©”ì„œë“œì™€ ì–´íœ˜ ëª©ë¡ì„ ì‚¬ìš©í•´ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” `transform()` ë©”ì„œë“œë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì¶œë ¥ì€ í¬ì†Œ í–‰ë ¬ì´ ë©ë‹ˆë‹¤.\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))\n",
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors\n",
    "X_few_vectors.toarray()\n",
    "ì´ í–‰ë ¬ì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”? ì„¸ ë²ˆì§¸ í–‰ì˜ ì²« ë²ˆì§¸ ì—´ì˜ 65ëŠ” ì„¸ ë²ˆì§¸ ì´ë©”ì¼ì´ ì–´íœ˜ ëª©ë¡ì— ì—†ëŠ” ë‹¨ì–´ë¥¼ 65ê°œ ê°€ì§€ê³  ìˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. ê·¸ ë‹¤ìŒì˜ 0ì€ ì–´íœ˜ ëª©ë¡ì— ìˆëŠ” ì²« ë²ˆì§¸ ë‹¨ì–´ê°€ í•œ ë²ˆë„ ë“±ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ëœ»ì´ê³  ê·¸ ë‹¤ìŒì˜ 1ì€ í•œ ë²ˆ ë‚˜íƒ€ë‚œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. ì´ ë‹¨ì–´ë“¤ì´ ë¬´ì—‡ì¸ì§€ í™•ì¸í•˜ë ¤ë©´ ì–´íœ˜ ëª©ë¡ì„ ë³´ë©´ ë©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë‹¨ì–´ëŠ” \"the\"ì´ê³  ë‘ ë²ˆì§¸ ë‹¨ì–´ëŠ” \"of\"ì…ë‹ˆë‹¤.\n",
    "vocab_transformer.vocabulary_\n",
    "ì´ì œ ìŠ¤íŒ¸ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œí‚¬ ì¤€ë¹„ë¥¼ ë§ˆì³¤ìŠµë‹ˆë‹¤! ì „ì²´ ë°ì´í„°ì…‹ì„ ë³€í™˜ì‹œì¼œë³´ì£ :\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)\n",
    "**Note**: to be future-proof, we set `solver=\"lbfgs\"` since this will be the default value in Scikit-Learn 0.22.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()\n",
    "98.5%ê°€ ë„˜ë„¤ìš”. ì²« ë²ˆì§¸ ì‹œë„ì¹˜ê³  ë‚˜ì˜ì§€ ì•ŠìŠµë‹ˆë‹¤! :) ê·¸ëŸ¬ë‚˜ ì´ ë°ì´í„°ì…‹ì€ ë¹„êµì  ì‰¬ìš´ ë¬¸ì œì…ë‹ˆë‹¤. ë” ì–´ë ¤ìš´ ë°ì´í„°ì…‹ì— ì ìš©í•´ ë³´ë©´ ê²°ê³¼ê°€ ê·¸ë¦¬ ë†’ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤. ì—¬ëŸ¬ê°œì˜ ëª¨ë¸ì„ ì‹œë„í•´ ë³´ê³  ì œì¼ ì¢‹ì€ ê²ƒì„ ê³¨ë¼ êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„¸ë°€í•˜ê²Œ íŠœë‹í•´ ë³´ì„¸ìš”.\n",
    "\n",
    "í•˜ì§€ë§Œ ì „ì²´ ë‚´ìš©ì„ íŒŒì•…í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ë©ˆì¶”ê² ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì •ë°€ë„/ì¬í˜„ìœ¨ì„ ì¶œë ¥í•´ ë³´ê² ìŠµë‹ˆë‹¤:\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"ì •ë°€ë„: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"ì¬í˜„ìœ¨: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3deada12ebf0dc721a2ad4cb99bb944b4b114c7bb123f3b6b4dd3ff7dfd28636"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
